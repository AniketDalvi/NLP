{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Aniket\n",
      "[nltk_data]     Dalvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package brown to C:\\Users\\Aniket\n",
      "[nltk_data]     Dalvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('brown')\n",
    "train = nltk.corpus.brown.tagged_sents(tagset='universal')[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# creating initial distribution dictionary\n",
    "oov_threshold = 2\n",
    "init_dist_dict = {}\n",
    "for sentence in train:\n",
    "    word = sentence[0][1]\n",
    "    if (word in init_dist_dict):\n",
    "        init_dist_dict[word] = init_dist_dict[word] + 1\n",
    "    else:\n",
    "        init_dist_dict[word] = 1\n",
    "\n",
    "init_dist = {}\n",
    "total_sent = len(train)\n",
    "for key, value in init_dist_dict.items():\n",
    "    init_dist[key] = (value + 0.001)/(total_sent + 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# pre-processsing pos tags in training data to create pi array and index map for pos\n",
    "pos_list = []\n",
    "pi = []\n",
    "for sentence in train:\n",
    "    for word in sentence:\n",
    "        pos_list.append(word[1])\n",
    "\n",
    "uniq_pos = list(set(pos_list))\n",
    "pos_index_map = {}\n",
    "for i in range(len(uniq_pos)):\n",
    "    pos_index_map[uniq_pos[i]] = i\n",
    "    if(uniq_pos[i] in init_dist):\n",
    "        pi.append(init_dist[uniq_pos[i]])\n",
    "    else:\n",
    "        pi.append(0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# creating transition matrix\n",
    "transition_counts = [ [0]*len(uniq_pos) for i in range(len(uniq_pos))]\n",
    "\n",
    "for sentence in train:\n",
    "    for j in range(len(sentence)-1):\n",
    "        x = pos_index_map[sentence[j][1]]\n",
    "        y = pos_index_map[sentence[j+1][1]]\n",
    "        transition_counts[x][y] = transition_counts[x][y] + 1\n",
    "\n",
    "transition_mat = [ [0]*len(uniq_pos) for i in range(len(uniq_pos))]\n",
    "\n",
    "for x in range(len(uniq_pos)):\n",
    "    for y in range(len(uniq_pos)):\n",
    "        transition_mat[x][y] = (transition_counts[x][y] + 0.001)/(sum(transition_counts[x]) + 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# pre-processsing words in training data to create index map for words\n",
    "all_words = []\n",
    "for sentence in train:\n",
    "    for words in sentence:\n",
    "        all_words.append(words[0].lower())\n",
    "\n",
    "uniq_words_oov = []\n",
    "words_index_map = {}\n",
    "word_freq = collections.Counter(all_words)\n",
    "for key in word_freq.keys():\n",
    "    if (word_freq[key] > oov_threshold):\n",
    "        uniq_words_oov.append(key)\n",
    "uniq_words_oov.append('oov')\n",
    "for i in range(len(uniq_words_oov)):\n",
    "    words_index_map[uniq_words_oov[i]] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# creating observation matrix\n",
    "observation_counts = [ [0]*len(uniq_words_oov) for i in range(len(uniq_pos))]\n",
    "for sentence in train:\n",
    "    for words in sentence:\n",
    "        x = pos_index_map[words[1]]\n",
    "        if(words[0].lower() not in uniq_words_oov):\n",
    "            y = words_index_map['oov']\n",
    "        else:\n",
    "            y = words_index_map[words[0].lower()]\n",
    "        observation_counts[x][y] = observation_counts[x][y] + 1\n",
    "\n",
    "observation_mat = [ [0]*len(uniq_words_oov) for i in range(len(uniq_pos))]\n",
    "for x in range(len(uniq_pos)):\n",
    "    for y in range(len(uniq_words_oov)):\n",
    "        observation_mat[x][y] = (observation_counts[x][y] + 0.001)/(sum(observation_counts[x]) + 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# implementing the viterbi algorithm\n",
    "def viterbi(obs, pi, a, b):\n",
    "    dp_matrix = [ [0]*len(obs) for i in range(len(uniq_pos))]\n",
    "    state_matrix = [ [0]*len(obs) for i in range(len(uniq_pos))]\n",
    "    for x in range(len(uniq_pos)):\n",
    "        for y in range(len(obs)):\n",
    "            if (y==0):\n",
    "                dp_matrix[x][y] = np.exp(np.log(pi[x]) + np.log(b[x][obs[y]]))\n",
    "                state_matrix[x][y] = -1\n",
    "\n",
    "    for y in range(1, len(obs)):\n",
    "        for x in range(len(uniq_pos)):\n",
    "            max_prob = float('-inf')\n",
    "            backtrack_state = -1\n",
    "            for z in range(len(uniq_pos)):\n",
    "                prob = np.exp(np.log(dp_matrix[z][y-1]) + np.log(a[z][x]) + np.log(b[x][obs[y]]))\n",
    "                if (prob>max_prob):\n",
    "                    max_prob = prob\n",
    "                    backtrack_state = z\n",
    "            dp_matrix[x][y] = (max_prob)\n",
    "            state_matrix[x][y] = backtrack_state\n",
    "    fin_max = float('-inf')\n",
    "    fin_state = -1\n",
    "    for j in range(len(uniq_pos)):\n",
    "        fin_prob = dp_matrix[j][len(obs)-1]\n",
    "        if(fin_prob>fin_max):\n",
    "            fin_max = fin_prob\n",
    "            fin_state = j\n",
    "    curr_best = fin_state\n",
    "    best_states = [curr_best]\n",
    "    for j in range(len(obs)-1,0,-1):\n",
    "        curr_best = state_matrix[curr_best][j]\n",
    "        best_states = [curr_best] + best_states\n",
    "\n",
    "    return best_states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case:\n",
      "[('Those', 'DET'), ('coming', 'VERB'), ('from', 'ADP'), ('other', 'ADJ'), ('denominations', 'NOUN'), ('will', 'VERB'), ('welcome', 'VERB'), ('the', 'DET'), ('opportunity', 'NOUN'), ('to', 'PRT'), ('become', 'VERB'), ('informed', 'VERB'), ('.', '.')]\n",
      "Predicted:\n",
      "['DET', 'VERB', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'VERB', '.']\n",
      "Test Case:\n",
      "[('The', 'DET'), ('preparatory', 'ADJ'), ('class', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('introductory', 'ADJ'), ('face-to-face', 'ADJ'), ('group', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('new', 'ADJ'), ('members', 'NOUN'), ('become', 'VERB'), ('acquainted', 'VERB'), ('with', 'ADP'), ('one', 'NUM'), ('another', 'DET'), ('.', '.')]\n",
      "Predicted:\n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'ADP', 'NUM', 'DET', '.']\n",
      "Test Case:\n",
      "[('It', 'PRON'), ('provides', 'VERB'), ('a', 'DET'), ('natural', 'ADJ'), ('transition', 'NOUN'), ('into', 'ADP'), ('the', 'DET'), ('life', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('local', 'ADJ'), ('church', 'NOUN'), ('and', 'CONJ'), ('its', 'DET'), ('organizations', 'NOUN'), ('.', '.')]\n",
      "Predicted:\n",
      "['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'CONJ', 'DET', 'NOUN', '.']\n",
      "Test Case:\n",
      "[('Reception', 'NOUN'), ('into', 'ADP'), ('the', 'DET'), ('Church', 'NOUN'), ('Fellowship', 'NOUN')]\n",
      "Predicted:\n",
      "['NOUN', 'ADP', 'DET', 'NOUN', 'NOUN']\n",
      "Test Case:\n",
      "[('the', 'DET'), ('total', 'NOUN'), ('process', 'NOUN'), ('of', 'ADP'), ('evangelism', 'NOUN'), ('reaches', 'VERB'), ('the', 'DET'), ('crescendo', 'NOUN'), ('when', 'ADV'), ('the', 'DET'), ('group', 'NOUN'), ('of', 'ADP'), ('new', 'ADJ'), ('members', 'NOUN'), ('stands', 'VERB'), ('before', 'ADP'), ('the', 'DET'), ('congregation', 'NOUN'), ('to', 'PRT'), ('declare', 'VERB'), ('publicly', 'ADV'), ('their', 'DET'), ('faith', 'NOUN'), ('and', 'CONJ'), ('to', 'PRT'), ('be', 'VERB'), ('received', 'VERB'), ('into', 'ADP'), ('the', 'DET'), ('fellowship', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('Church', 'NOUN'), ('.', '.')]\n",
      "Predicted:\n",
      "['DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADV', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'PRT', 'VERB', 'ADV', 'DET', 'NOUN', 'CONJ', 'PRT', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n"
     ]
    }
   ],
   "source": [
    "# testing pos tagging on sample data using viterbi\n",
    "test = nltk.corpus.brown.tagged_sents(tagset='universal')[10150:10155]\n",
    "transition_mat = np.array(transition_mat)\n",
    "observation_mat = np.array(observation_mat)\n",
    "test_sentences = []\n",
    "for sentence in test:\n",
    "    test_sentence = []\n",
    "    for words in sentence:\n",
    "        if(words[0].lower() not in uniq_words_oov):\n",
    "            test_sentence.append(words_index_map['oov'])\n",
    "        else:\n",
    "            test_sentence.append(words_index_map[words[0].lower()])\n",
    "    test_sentences.append(test_sentence)\n",
    "\n",
    "index_pos_map = {y:x for x,y in pos_index_map.items()}\n",
    "predicted_states = []\n",
    "for test_sentence in test_sentences:\n",
    "    predicted_states.append([index_pos_map[x] for x in viterbi(test_sentence, pi, transition_mat, observation_mat)])\n",
    "\n",
    "index_test = 0\n",
    "for raw_test in test:\n",
    "    print('Test Case:')\n",
    "    print(raw_test)\n",
    "    print('Predicted:')\n",
    "    print(predicted_states[index_test])\n",
    "    index_test += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}